\section{Backpropagation in the Simply Typed Lambda-Calculus with Linear Negation}
Backpropagation is a classic automatic diﬀerentiation algorithm computing the gradient of functions speciﬁed by a certain class of simple, ﬁrst-order programs, called \emph{computational graphs}. 
Recent years have witnessed the quick growth of a research ﬁeld called \emph{diﬀerentiable programming}, the aim of which is to express computational graphs more synthetically and modularly by resorting to actual programming languages endowed with control ﬂow operators and higher-order combinators, such as map and fold. In this paper, we extend the backpropagation algorithm to a paradigmatic example of such a programming language: we deﬁne a compositional program transformation from the simply-typed lambda-calculus to itself augmented with a notion of linear negation, and prove that this computes the gradient of the source program with the same eﬃciency as ﬁrst-order backpropagation.

\subsection{Introduction}
In the past decade there has been a surge of interest in so-called deep learning, a class of machine learning methods based on multi-layer neural networks. The term “deep” has no formal meaning, it is essentially a synonym of “multi-layer”, which refers to the fact that the networks have, together with their input and output layers, at least one internal (or “hidden”) layer of artiﬁcial neurons. Technically, the neurons are just functions $\mathbb{R}^n \rightarrow \mathbb{R}$ of the form $(x_1,..., x_m) \rightarrow \sigma(\sum^{m}_{i=1} w_i \cdot x_i)$, where $\sigma: \mathbb{R}\rightarrow\mathbb{R}$ is some \emph{activation function} and $w_1,..., w_m \in \mathbb{R}$ are the \emph{weights} of the neuron.

Feed-forward, multi-layer neural networks are known to be universal approximators: any continuous function $f: K \rightarrow \mathbb{R}$ with $K \subset \mathbb{R}^k$ compact may be approximated to an arbitrary degree of precision by a feed-forward neural network with one hidden layer, as long as the weights are set properly. This leads us to the following two questions related to each other:
\begin{enumerate}
	\item How to eﬃciently train a neural network? i.e., how to ﬁnd the right weights as quickly as possible?
	\item How to select and, if necessary, modify or design network architectures adapted to a given task?
	Since the quality of an architecture is also judged in terms of training eﬃciency, this problem is actually interlaced
	with the previous one.
\end{enumerate}

\subsubsection{How to eﬃciently train a neural network?} The ﬁrst question is generally answered in terms of the gradient descent algorithm (or some variant thereof). 

\paragraph{Gradient Descent Algorithm} The Gradient Descent Algorithm ﬁnds local minima of a function $G : \mathbb{R}^n \rightarrow \mathbb{R}$ using its gradient $\nabla G$. The algorithm starts by choosing a point $\mathbf{w}_0 \in \mathbb{R}^n$. Under certain assumptions, if $\nabla G(\mathbf{w}_0)$ is close to zero then $\mathbf{w}_0$ is within a suﬃciently small neighborhood of a local minimum. Otherwise, we know that $G$ decreases most sharply in the opposite direction of $\nabla G(\mathbf{w}_0)$, and so the algorithm sets $\mathbf{w}_1 := \mathbf{w}_0 - \rho \nabla G(\mathbf{w}_0)$ for a suitable step rate $\rho >0$, and repeats the procedure from $\mathbf{w}_1$ .

\paragraph{AD and efficient training of neural networks} So, regardless of the architecture, eﬃciently training a neural network involves eﬃciently computing gradients. The interest of gradient descent, however, goes well beyond deep learning, into ﬁelds such as physical modeling and engineering design optimization, each with numerous applications. It is thus no wonder that a whole research ﬁe
known as automatic diﬀerentiation (AD for short), developed around the computation of gradients. In AD, the setting of neural networks is generalized to computational graphs, which are arbitrarily complex compositions of nodes computing basic functions and in which the output of a node may be shared as the input of an unbounded number of nodes.

The key idea of AD is to compute the gradient of a computational graph by accumulating in a suitable way the
partial derivatives of the basic functions composing the graph. This rests on the mathematical principle known as
\emph{chain rule}, giving the derivative of a composition $f \circ g$ from the derivatives of its components $f$ and $g$. Formally, the chain rule states that

$$\forall r \in \mathbb{R} \quad \quad (g \circ f)' (r) = g'(f(r)) \cdot f'(r)$$

There are two main “modes” of applying this rule in AD, either forward, propagating derivatives from inputs to outputs, or backwards, propagating derivatives from outputs to inputs. If $G$ is a computational graph with $n$ inputs and $m$ outputs invoking $|G|$ operations, forward mode computes the Jacobian of $G$ in $O(n |G|)$ operations, while reverse mode requires $O(m |G|)$ operations. In deep learning, as the number of layers increases, $n$ becomes astronomical (millions, or even billions) while $m = 1$, hence the reverse mode is the method of choice and specializes in what is called the backpropagation algorithm. Today, AD techniques are routinely used in the industry through deep learning frameworks such as TensorFlow and PyTorch.


\subsubsection{How to select and modify or design network architectures adapted to a given task?}
The interest of the programming languages (PL) community in AD stems from the second deep learning question mentioned above, namely the design and manipulation of (complex) neural network architectures. As it turns out, these are being expressed more and more commonly in terms of actual programs. Although these programs always reduce, in the end, to computational graphs, these latter are inherently static and therefore inadequate to properly describe something which is in reality, a dynamically-genereted neural network.

In PL-theoretic terms, this amounts to fixing a reduction strategy, which cannot always be optimal in terms of efficiency. There is also a question of modularity: if gradients may only be computed by running programs, then we are implicitly rejecting the possibility of computing gradients modularly, because a minor change in the code might result in having to recompute everything from scratch, which is clearly unsatisfactory.

\subsubsection{Goal}
Define a compositional program trasformation $\stackrel{\leftarrow}{D}$ extending the backpropagation algorithm from computational graph to general simply typed $\lambda$-terms. Our framework is purely logical and therefor offer the possibility of importing tool from semantics, type systems and rewriting theory.

\subsection{A Crash Course in Automatic Differentiation}

\subsubsection{What is Automatic Differentiation?}
Automatic differentiation (or AD) is the science of efficientlly computing the derivative of (a restricted class of) programs. Such programs may be represented as directed acyclic hypergraphs, called \emph{computational graphs}, whose nodes are variables of type $\mathbb{R}$ and whose hyperedges are labelled by functions drawn from some finite set of interest with the restriction that hyperedges have exactly one targetnode and that each node is the target of at most one hyperedge. The basic idea is that nodes that are not target of any hyperedge represent input variables, nodes which are not source of any hyperedge represent outputs, and a hyperedge $x_1, \dots, x_k \xrightarrow{f} y$ represents an assignment $y:= f(x_1,\dots, x_k)$. So that a computational graph with $n$ inputs and $m$ outputs represents a function of type $\mathbb{R}^n\rightarrow \mathbb{R}^m$. 

In terms of programming languages, we may define computational graphs as generated by
$$ F, G ::= x \mbox{ }|\mbox{ }  f(x_1,...,x_k) \mbox{ }|\mbox{ }  \mathtt{let} \mbox{ } x = G \mbox{ } \mathtt{in} \mbox{ }F \mbox{ }|\mbox{ }  (F,G) $$

where $x$ range over ground variables and $f$ over a set of real function symbols. The $\mathtt{let}$ binders are necessary to represents \emph{sharing}. The set of real function symbols consists of one nullary symbol for every real number plus finitely many non-nullary symbols for actual functions. 

\paragraph{Example} For example, the computational graph that represents the function $(x_1, x_2) \mapsto sin((x_1-x_2)^2)$ and its corresponding term are described in Figure~\ref{compGraph2}.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.5]{img/compGraph2}
	\caption{Computational graph of the function $f(x_1,x_2)=sin((x_1-x_2)^2)$ and its corresponding term. Nodes are drawn as circles, 			hyperedges as triangles.}
	\label{compGraph2}
\end{figure}

We may write $f(G_1,\dots,G_n)$ as a syntactic sugar for $\mathtt{let} \mbox{ } x_1 = G_1 \mbox{ } \mathtt{in} \mbox{ } \dots \mbox{ } \mathtt{let} \mbox{ } x_n = G_n \mbox{ } \mathtt{in}  \mbox{ } \\ f(x_1,...,x_n)$.  Typing is as expected: types are of the form $R^k$ and every computational graph in context $x_1:R, \dots, x_n:R \vdash G: R^m$ denotes a function $\llbracket G \rrbracket: \mathbb{R}^n \rightarrow \mathbb{R}^m$. Restricting for semplicity to the one-output case, we may say that, as far as we are concerned, the central question of AD is computing the gradient of $\llbracket G \rrbracket$ at any point $r \in \mathbb{R}^n$, as efficiently as possible. Of course, this question make sense only if $\llbracket G \rrbracket$ is differentiable, which is the case if every function symbol represents a differentiable function. In practice this is not always guaranteed but this is actually an issue. In terms of hypergraphs, we are given a computational graph $G$ with input nodes $x_1, \dots, x_n$ together with an assigment $x_i=r_i$ with $r_i \in \mathbb{R}$ for all $1 \leq i \leq n$. The value $\llbracket G \rrbracket (r_1, \dots, r_n)$ is found by progressively computing the assigments $w := f(s_1,\dots, s_m)$ for each hyperedges $z_1, \dots, z_m \xrightarrow{f} w$ such that the value of all $z_i$ are already known. This is known as \emph{forward evaluation} and has cost $|G|$ (the number of nodes of G).

\subsubsection{Forward Mode AD}
The simplest AD algorithm is known as \emph{forward mode differentiation}. Suppose that we are given a computational graph $G$ with input nodes $x_1, \dots, x_n$ and one output node $y$, and suppose that we want to compute its j-th partial derivative in $\mathbf{r}=(r_1,\dots,r_n)\in \mathbb{R}^n$. The algorithm maintains a memory consisting of a set of assigments of the form $x:=(s,t)$, where $x$ is a node of $G$ and $s,t \in \mathbb{R}$ (known as \emph{primal} and \emph{tangent}) and proceeds as follows

\begin{itemize}
	\item We initialize the memory with $x_i :=(r_i, 0)$ for all $1 \leq i \leq n, i \neq j$, and $x_j :=(r_j, 1)$

	\item Let us consider all the nodes $z_1, \dots, z_k $ that are used to calculate the value of a node $w$ by applying the function $f$, 			the pair associated with $w$ is obtained as follows:
	$$w:= \bigg(  f(\mathbf{s}) , \sum^k_{i=1} \partial_i f(\mathbf{s}) \cdot t_i \bigg)$$
\end{itemize}

\paragraph{Example} For example, if $G$ is the computational graph described above, we obtain the forward mode described in Figure~\ref{compGraph2FM} which is what we expect since $\partial_i \llbracket G \rrbracket (x_1,x_2)= cos((x_1-x_2)^2) \cdot 2(x_1-x_2)$.
\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.5]{img/compGraph2FM}
	\caption{Forward mode AD of the function $f(x_1,x_2)=sin((x_1-x_2)^2)$}
	\label{compGraph2FM}
\end{figure}

\paragraph{Complexity} Since the arity k of function symbols is bounded, the cost of computing one partial derivative is $O(|G|)$. Computing the gradient requires computing all $n$ partial derivatives, giving of a total cost of $O(n |G|)$, which is not very eﬃcient
since $n$ may be huge (typically, it is the number of weights of a deep neural network, which may well be in the millions).

\subsubsection{Symbolic AD}
The basic idea of symbolic AD is to generate, starting from a computational graph $G$ with $n$ input nodes and $1$ output node, a computational graph $\stackrel{\rightarrow}{D}(G)$ with $2n$ input nodes and 2 output nodes such that forward evaluation of $\stackrel{\rightarrow}{D}(G)$ corresponds to executing forward mode AD on $G$, i.e., for all $\mathbf{r}= r_1, \dots, r_n \in \mathbb{R}$, the output of $\stackrel{\rightarrow}{D}(G)(r_1,0,\dots, r_j,1, \dots,r_n,0)$ is $(\llbracket G \rrbracket(\mathbf{s}), \partial_j \llbracket G \rrbracket(\mathbf{s}))$.

From the programming languages standpoint, symbolic AD is interesting because:
\begin{enumerate} 
	\item it allows one to perform optimizations on $\stackrel{\rightarrow}{D}(G)$ which would be inaccessible when simply running the 			algorithm on $G$
	\item it opens the way to compositionality,
	\item being a (compositional) program transformation rather than an algorithm, it oﬀers a viewpoint from which
	AD may possibly be extended beyond computational graphs.
\end{enumerate}

\subsubsection{Reverse Mode AD or Backpropagation}
A more eﬃcient way of computing gradients in the many inputs/one output case is provided by reverse mode automatic
diﬀerentiation, from which the backpropagation (often abbreviated as backprop) algorithm derives. As usual, we are
given a computational graph (seen as a hypergraph) $G $with input nodes $x_1 , \dots , x_n$ and output node $y$, as well as
$\mathbf{r}= r_1 , \dots , r_n \in \mathbb{R}$ which is the point where we wish to compute the gradient. The backprop algorithm maintains a memory consisting of assignments of the form $x := (r, \alpha)$, where $x$ is a node of $G$ and $r, \alpha \in \mathbb{R}$ (the \emph{primal} and the \emph{adjoint}), plus a boolean ﬂag with values “marked/unmarked” for each hyperedge of $G$, and proceeds thus:\\ 

\noindent \textbf{initialization}: the memory is initialized with $x_i := (r_ i , 0)$ for all $1 \leq i \leq n$, and the forward phase starts;\\

\noindent\textbf{forward phase}: at each step, a new assignment $z := (s, 0)$ is produced, with $s$ being computed exactly as during
forward evaluation, ignoring the second component of pairs in memory (i.e., s is the value of node z); once
every node of $G$ has a corresponding assignment in memory, the assignment $y := (t, 0)$ is updated to $y := (t, 1)$,
every hyperedge is ﬂagged as unmarked and the backward phase begins;\\

\noindent\textbf{backward phase}: at each step, we look for an unmarked hyperedge $z _1 ,\dots, z_k \xrightarrow{f} w$ such that all hyperedges having $w$ among their sources are marked. If no such hyperedge exists, we terminate. Otherwise, assuming
that the memory contains $w := (u, \alpha)$ and $z_i := (s_i , \beta_i )$ for all $1 \leq i \leq k$, we update the memory with the
assignments $z_i := (s_i , \beta_i + \partial_i f(\mathbf{s}) \cdot \alpha)$ (where $\mathbf{s}:= s_1 , \dots , s_k$ ) for all $1 \leq i \leq k$ and ﬂag the hyperedge as marked.

\paragraph{Example} For example, if $G$ is the computational graph described above, we obtain the initialization and the forward phase as described in Figure~\ref{compGraph2IFM}.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.5]{img/compGraph2IFM}
	\caption{Initialization and forward phase of the function $f(x_1,x_2)=sin((x_1-x_2)^2)$}
	\label{compGraph2IFM}
\end{figure}

Then we compute the backward phase as described in Figure~\ref{compGraph2BM}.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.5]{img/compGraph2BM}
	\caption{Backward phase of the function $f(x_1,x_2)=sin((x_1-x_2)^2)$}
	\label{compGraph2BM}
\end{figure}

\paragraph{Complexity}
By construction, the backward phase scans each hyperedge exactly once performing each time a constant number
of operations. So both phases are linear in $|G|$, giving a total cost of $O(|G|)$, like forward mode. Except that, unlike
forward mode, a single evaluation now already gives us the whole gradient, independently of the number of inputs!

\subsubsection{Symbolic Backpropagation and the Compositionality Issue}
The symbolic methodology we saw for forward mode AD may be applied to the reverse mode too: given a computational graph $G$ with $n$ inputs and one output, one may produce a computational graph $\mathbf{bp}(G)$ with $n + 1$ inputs and $1 + n$ outputs such that, for all $\mathbf{r} = r_1 , \dots , r_n \in \mathbb{R}$, the forward evaluation of $\mathbf{bp}(G)(\mathbf{r}, 1)$ has output $(\llbracket G \rrbracket(\mathbf{r} ), \nabla\llbracket G \rrbracket(\mathbf{r} ))$. Moreover, $\mathbf{bp}(G) = O(|G|)$.

\paragraph{Example} Let us look at an example, shown in Figure~\ref{compGraph2SBM}. First of all, observe that $\mathbf{bp}_{x_1, x_2, a} (G)$ contains a copy of $G$, marked in blue. This corresponds to the forward phase. The nodes marked in red correspond to the backward phase.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.45]{img/compGraph2SBM}
	\caption{The computational graph $\mathbf{bp}_{x_1, x_2, a} (G)$ where $G$ is in Figure~\ref{compGraph2}, and its corresponding 			term}
	\label{compGraph2SBM}
\end{figure}

Note that the the forward evaluation of $\mathbf{bp}_{x_1, x_2, a} (G)$ with $x_1 := 5, x_2 := 2$ and $a := 1$ matches exactly the steps of the backprop algorithm.

\paragraph{Compositionality Issue}
Rather than examining the details of the deﬁnition of $\mathbf{bp}$, let us observe at once that, from the standpoint of programming languages, it suﬀers from a serious defect: it is not compositional.